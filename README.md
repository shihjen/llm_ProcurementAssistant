![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red)
![Langchain](https://img.shields.io/badge/Framework-Langchain-orange)
![Local LLM](https://img.shields.io/badge/LLM-Ollama-green)
![License](https://img.shields.io/badge/License-MIT-yellow)


## ğŸ“„ Smart Procurement Assistant (Local LLM)

A lightweight AI-assisted procurement application that uses a locally hosted Large Language Model (LLM) to extract, structure, and compare vendor quotations against procurement requirements.

The application runs fully locally using Ollama, without requiring external API services.

### ğŸš© Project Overview

Procurement evaluation often involves manually reviewing specification documents and vendor quotations in different formats (PDF, Word, Excel).
This project demonstrates how a local LLM can assist in:

- converting procurement specifications into structured requirements
- extracting structured information from vendor quotations
- comparing vendor compliance against requirements
- generating a simple recommendation summary

The goal of this project is workflow augmentation, not full automation.

### âš™ï¸ Features

![feature](assets/img/feature.jpg)
*Image generated by Nano Banana*

- ğŸ“„ Upload procurement specifications (PDF / DOCX / XLSX)
- ğŸ§  Automatic requirement schema generation using LLM
- ğŸ“‘ Upload multiple vendor quotations
- ğŸ” Information extraction from unstructured documents
- ğŸ“Š Rule-based vendor compliance evaluation
- ğŸ¤– AI-generated recommendation summary
- ğŸ”’ Fully local execution (no cloud API required)

### ğŸ“ Project Structure

```
.
â”œâ”€â”€ app.py
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ schema_generator.py
â”‚   â”œâ”€â”€ model_builder.py
â”‚   â”œâ”€â”€ quotation_extractor.py
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â””â”€â”€ recommender.py
â”‚
â”œâ”€â”€ loaders/
â”‚   â””â”€â”€ document_loaders.py
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ json_parser.py

```


### ğŸš€ Installation

1. Clone repository
```
git clone https://github.com/shihjen/llm_ProcurementAssistant.git
cd llm_ProcurementAssistant
```

2. Create virtual environment
```
python -m venv .venv
.venv/Scripts/activate
```

3. Install dependencies
```
pip install -r requirements.txt
```

4. Install and run Ollama

Install Ollama:

ğŸ‘‰ https://ollama.com

Pull a local model:
```
ollama pull gpt-oss
```

Start Ollama service before running the app.

5. Run application
```
streamlit run app.py
```

### ğŸ§ª Process Workflow

![workflow](assets/img/workflow.jpg)
*Image generated by Nano Banana*

1. Upload procurement specification
2. Generate requirement schema
3. Upload vendor quotation documents
4. Review compliance comparison
5. Generate recommendation summary


### âš ï¸ Limitations

- Extraction accuracy depends on document quality and model capability
- Local LLMs may occasionally produce inconsistent JSON outputs
- Evaluation logic currently assumes simplified requirement comparison
- Not intended for production procurement decisions

This project is intended for learning and experimentation with local LLM workflows.
